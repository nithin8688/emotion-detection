{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb6cab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Visual Studio practice\\emotion_detection\\venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Visual Studio practice\\emotion_detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_DIR = \"../models\"\n",
    "model_path = os.path.join(MODEL_DIR, \"emotion_model.h5\")\n",
    "model = load_model(model_path)\n",
    "\n",
    "label_map = {0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'neutral', 5:'sad', 6:'surprise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a68a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Visual Studio practice\\emotion_detection\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "def predict_emotion_from_real_photo(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"⚠️ Could not load image at {image_path}\")\n",
    "\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    faces = detector.detect_faces(rgb_img)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        print(\"⚠️ No face detected in image.\")\n",
    "        return\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, w, h = face['box']\n",
    "        roi = rgb_img[y:y+h, x:x+w]\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
    "        roi = cv2.resize(roi, (48,48))\n",
    "        roi = roi.astype(\"float32\") / 255.0\n",
    "        roi = np.expand_dims(roi, axis=(0,-1))\n",
    "\n",
    "        preds = model.predict(roi)\n",
    "        label = label_map[np.argmax(preds)]\n",
    "        print(f\"✅ Predicted emotion: {label}\")\n",
    "\n",
    "        # Draw bounding box + label\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "        cv2.putText(img, label, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Real Photo Prediction\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314323ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "✅ Predicted emotion: angry\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(os.path.exists(r\"D:\\Visual Studio practice\\emotion_detection\\Images\"))\n",
    "predict_emotion_from_image(r\"D:\\Visual Studio practice\\emotion_detection\\Images\\angry_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed47e66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Use Haar cascade to detect faces\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "        roi = cv2.resize(roi, (48,48))\n",
    "        roi = roi.astype(\"float32\") / 255.0\n",
    "        roi = np.expand_dims(roi, axis=(0,-1))\n",
    "\n",
    "        # Predict emotion\n",
    "        preds = model.predict(roi)\n",
    "        label = label_map[np.argmax(preds)]\n",
    "\n",
    "        # Draw bounding box + label\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "        cv2.putText(frame, label, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key in [ord('q'), ord('e'), ord('z')]:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a417c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
